# MLP Simples - Documenta√ß√£o

√â um algoritmo de rede neural que recebe em 3 camadas diferentes, par√¢metros, para aprendizado, e assim pode devolver essas sa√≠das de resultados do c√°lculo desses par√¢metros, al√©m de um c√°lculo decimal de erros, para que possamo identificar evolu√ß√£o conforme o tempo, que √© chamado de √©pocas no algoritmo.

Usando apenas NumPy, e √© capaz de aprender a resolver o problema l√≥gico da porta XOR.

---

## üß† Implementa√ß√£o de Rede Neural MLP em Python

Este projeto implementa uma **rede neural MLP (Multilayer Perceptron)** simples do zero usando apenas `NumPy`, e √© capaz de aprender a resolver o problema l√≥gico da **porta XOR**.

---

### üîç Como funciona o c√≥digo

#### 1. **Fun√ß√£o de Ativa√ß√£o: `sigmoid`**
```python
def sigmoid(x):
    return 1 / (1 + np.exp(-x))
```
A `sigmoid` transforma a sa√≠da dos neur√¥nios em valores entre 0 e 1. Ela introduz **n√£o linearidade**, essencial para resolver problemas como XOR.

#### 2. **Derivada da Sigmoid: `sigmoid_derivative`**
```python
def sigmoid_derivative(x):
    return x * (1 - x)
```
Usada no c√°lculo do gradiente durante o **backpropagation**, ajuda a ajustar os pesos com base no erro.

---

### üèóÔ∏è Classe `NeuralNetwork`

#### **`__init__`: Inicializa os par√¢metros**
```python
self.weights = []
```
- Define o n√∫mero de camadas (entrada, ocultas, sa√≠da).
- Inicializa os **pesos aleat√≥rios** entre as camadas.

---

#### **`feedforward`: Propaga√ß√£o para frente**
```python
for weight in self.weights:
    ...
```
- Calcula as ativa√ß√µes de cada camada.
- Passa a entrada adiante, camada por camada, at√© obter a sa√≠da.

---

#### **`backpropagation`: Ajuste dos pesos**
```python
error = y_true - activations[-1]
```
- Calcula o erro da previs√£o.
- Propaga esse erro **de tr√°s para frente** (backpropagation).
- Usa o erro e a derivada da sigmoid para atualizar os pesos.

---

#### **`train`: Treinamento da rede**
```python
for epoch in range(self.epochs):
```
- Repetidamente executa `feedforward` e `backpropagation` para cada amostra.
- Opcionalmente imprime o erro a cada 1000 √©pocas.

---

#### **`predict`: Faz previs√µes**
```python
def predict(self, X):
```
- Realiza apenas a propaga√ß√£o para frente para obter a sa√≠da final.

---

### üß™ Exemplo: Resolviendo o problema XOR

```python
X = np.array([[0,0],[0,1],[1,0],[1,1]])
y = np.array([[0],[1],[1],[0]])
```

- Entradas e sa√≠das do problema XOR.
- A rede aprende o padr√£o e consegue prever corretamente os resultados.

---

### ‚öôÔ∏è Par√¢metros escolhidos

```python
nn = NeuralNetwork(input_size=2, hidden_sizes=[3], output_size=1, learning_rate=0.9, epochs=100000)
```

- **2 neur√¥nios de entrada** (XOR usa dois bits).
- **1 camada oculta com 3 neur√¥nios**.
- **1 neur√¥nio de sa√≠da** (0 ou 1).
- **Taxa de aprendizado**: 0.9.
- **√âpocas**: 100 mil ciclos de treino.

---

### üìå Resultado Esperado

```text
Entrada: [0 0], Sa√≠da Prevista: [0.]
Entrada: [0 1], Sa√≠da Prevista: [1.]
Entrada: [1 0], Sa√≠da Prevista: [1.]
Entrada: [1 1], Sa√≠da Prevista: [0.]
```

A rede aprende corretamente o comportamento da fun√ß√£o XOR.

---
