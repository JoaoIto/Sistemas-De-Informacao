# Aula 08 ‚Äì Perceptron: Ajuste de Pesos

Nesta aula, exploramos como ocorre o **ajuste de pesos em um perceptron**, um ponto fundamental no aprendizado supervisionado.

### T√≥picos discutidos:

1. **Funcionamento do Perceptron Simples**:

   * Revisamos o modelo do perceptron proposto por McCulloch e Pitts.
   * Discutimos como as entradas, pesos e bias se combinam para produzir uma sa√≠da.
   * Vimos a implementa√ß√£o em **Python** de um perceptron simples, observando passo a passo o fluxo do algoritmo.

2. **Algoritmo de Ajuste de Pesos**:

   * Apresentamos o algoritmo que permite ao perceptron "aprender" a partir de exemplos.
   * O peso √© ajustado usando a f√≥rmula:

     $$
     w_i \leftarrow w_i + \eta (y - \hat{y}) x_i
     $$

     Onde:

     * $\eta$ √© a taxa de aprendizado,
     * $y$ √© a sa√≠da esperada,
     * $\hat{y}$ √© a sa√≠da predita pelo perceptron,
     * $x_i$ √© a entrada correspondente.

3. **Limita√ß√£o do Perceptron com XOR**:

   * Mostramos que o perceptron simples **n√£o consegue resolver o problema l√≥gico XOR**, pois este **n√£o √© linearmente separ√°vel**.
   * Essa limita√ß√£o foi o que motivou o avan√ßo para redes neurais mais complexas.

4. **Solu√ß√£o com M√∫ltiplos Perceptrons**:

   * Para resolver o XOR, apresentamos como usar **tr√™s perceptrons em conjunto**, formando uma **rede neural simples com uma camada oculta**.
   * Isso introduz a ideia do **Perceptron Multicamadas (MLP)**.

---

## Grava√ß√µes da Aula

* üîπ **Parte 1 ‚Äì 29/03/2025**
  *Funcionamento do perceptron simples e algoritmo de ajuste de pesos*
  [link](https://drive.google.com/file/d/1746w8xr2ks3IGrrjuFDXdgf-9z-dDYOd/view)
* üîπ **Parte 2 ‚Äì 29/03/2025**
  *Limita√ß√µes do perceptron simples e solu√ß√£o com m√∫ltiplos perceptrons*
  [link](https://drive.google.com/file/d/1gpl3CeYzil0fXICMNlUJ0dmfE_IVAhvV/view)

---

# Correla√ß√£o entre Diagrama e C√≥digo Perceptron

Para refor√ßar a compreens√£o, foi apresentada uma tabela de correspond√™ncia entre os **elementos do diagrama cl√°ssico de McCulloch & Pitts** e o **c√≥digo Python implementado em aula**:

| Elemento do Diagrama (McCulloch & Pitts) | Parte correspondente no c√≥digo Python |
| ---------------------------------------- | ------------------------------------- |
| Entradas $x_1, x_2, ..., x_n$            | `self.X` (lista de entradas)          |
| Pesos $w_1, w_2, ..., w_n$               | `self.weights`                        |
| Bias (ou limiar Œ∏)                       | `self.bias`                           |
| Fun√ß√£o somadora (Œ£)                      | `np.dot(self.X, self.weights)`        |
| Fun√ß√£o de ativa√ß√£o                       | `step_function()`                     |
| Sa√≠da $y$                                | Resultado de `predict()`              |
| Atualiza√ß√£o de pesos                     | La√ßo `for` com `self.weights += ...`  |

A imagem abaixo mostra a liga√ß√£o entre o **diagrama visual**, a **tabela de correspond√™ncia** e o **c√≥digo Python**:
![image](https://github.com/user-attachments/assets/2363e5c3-e8d2-497a-a4b1-9ab1c0042596)

![Imagem: Diagrama vs C√≥digo Perceptron](attachment:/mnt/data/f8cc312b-c784-4ea1-8a69-146bbb950b25.png)

---

